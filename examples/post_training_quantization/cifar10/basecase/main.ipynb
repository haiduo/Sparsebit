{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-trainin-Quantization for CIFAR10\n",
    "We will do the following steps in order\n",
    "1. Load and Normalize CIFAR10\n",
    "2. Define Quant Model\n",
    "3. Load Pretrained Model and Test\n",
    "4. Calibration\n",
    "5. Test Quant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关的库函数\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from model import resnet20\n",
    "\n",
    "from sparsebit.quantization import QuantModel, parse_qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load and Normalize CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Load Pretrained Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Float Model on the 10000 test images: 91.42 %\n"
     ]
    }
   ],
   "source": [
    "# 模型采用resnet20\n",
    "model = resnet20(num_classes=10)\n",
    "\n",
    "PATH = \"./pretrain_model.pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# 在float模型上进行精度的测试\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        image,labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            image,labels = image.cuda(),labels.cuda()\n",
    "        outputs = model(image)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "pretrain_acc1 = 100 * correct / total \n",
    "print(f'Accuracy of the Float Model on the 10000 test images: {pretrain_acc1} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define QuantModel\n",
    "- use API from sparsebit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                   target                   args                                     kwargs\n",
      "-------------  ---------------------  -----------------------  ---------------------------------------  --------\n",
      "placeholder    x                      x                        ()                                       {}\n",
      "call_module    conv1                  conv1                    (x,)                                     {}\n",
      "call_module    relu                   relu                     (conv1,)                                 {}\n",
      "call_module    layer1_0_conv1         layer1.0.conv1           (relu,)                                  {}\n",
      "call_module    layer1_0_relu          layer1.0.relu            (layer1_0_conv1,)                        {}\n",
      "call_module    layer1_0_conv2         layer1.0.conv2           (layer1_0_relu,)                         {}\n",
      "call_function  add                    <built-in function add>  (layer1_0_conv2, relu)                   {}\n",
      "call_module    layer1_0_relu_1        layer1.0.relu            (add,)                                   {}\n",
      "call_module    layer1_1_conv1         layer1.1.conv1           (layer1_0_relu_1,)                       {}\n",
      "call_module    layer1_1_relu          layer1.1.relu            (layer1_1_conv1,)                        {}\n",
      "call_module    layer1_1_conv2         layer1.1.conv2           (layer1_1_relu,)                         {}\n",
      "call_function  add_1                  <built-in function add>  (layer1_1_conv2, layer1_0_relu_1)        {}\n",
      "call_module    layer1_1_relu_1        layer1.1.relu            (add_1,)                                 {}\n",
      "call_module    layer1_2_conv1         layer1.2.conv1           (layer1_1_relu_1,)                       {}\n",
      "call_module    layer1_2_relu          layer1.2.relu            (layer1_2_conv1,)                        {}\n",
      "call_module    layer1_2_conv2         layer1.2.conv2           (layer1_2_relu,)                         {}\n",
      "call_function  add_2                  <built-in function add>  (layer1_2_conv2, layer1_1_relu_1)        {}\n",
      "call_module    layer1_2_relu_1        layer1.2.relu            (add_2,)                                 {}\n",
      "call_module    layer2_0_conv1         layer2.0.conv1           (layer1_2_relu_1,)                       {}\n",
      "call_module    layer2_0_relu          layer2.0.relu            (layer2_0_conv1,)                        {}\n",
      "call_module    layer2_0_conv2         layer2.0.conv2           (layer2_0_relu,)                         {}\n",
      "call_module    layer2_0_downsample_0  layer2.0.downsample.0    (layer1_2_relu_1,)                       {}\n",
      "call_function  add_3                  <built-in function add>  (layer2_0_conv2, layer2_0_downsample_0)  {}\n",
      "call_module    layer2_0_relu_1        layer2.0.relu            (add_3,)                                 {}\n",
      "call_module    layer2_1_conv1         layer2.1.conv1           (layer2_0_relu_1,)                       {}\n",
      "call_module    layer2_1_relu          layer2.1.relu            (layer2_1_conv1,)                        {}\n",
      "call_module    layer2_1_conv2         layer2.1.conv2           (layer2_1_relu,)                         {}\n",
      "call_function  add_4                  <built-in function add>  (layer2_1_conv2, layer2_0_relu_1)        {}\n",
      "call_module    layer2_1_relu_1        layer2.1.relu            (add_4,)                                 {}\n",
      "call_module    layer2_2_conv1         layer2.2.conv1           (layer2_1_relu_1,)                       {}\n",
      "call_module    layer2_2_relu          layer2.2.relu            (layer2_2_conv1,)                        {}\n",
      "call_module    layer2_2_conv2         layer2.2.conv2           (layer2_2_relu,)                         {}\n",
      "call_function  add_5                  <built-in function add>  (layer2_2_conv2, layer2_1_relu_1)        {}\n",
      "call_module    layer2_2_relu_1        layer2.2.relu            (add_5,)                                 {}\n",
      "call_module    layer3_0_conv1         layer3.0.conv1           (layer2_2_relu_1,)                       {}\n",
      "call_module    layer3_0_relu          layer3.0.relu            (layer3_0_conv1,)                        {}\n",
      "call_module    layer3_0_conv2         layer3.0.conv2           (layer3_0_relu,)                         {}\n",
      "call_module    layer3_0_downsample_0  layer3.0.downsample.0    (layer2_2_relu_1,)                       {}\n",
      "call_function  add_6                  <built-in function add>  (layer3_0_conv2, layer3_0_downsample_0)  {}\n",
      "call_module    layer3_0_relu_1        layer3.0.relu            (add_6,)                                 {}\n",
      "call_module    layer3_1_conv1         layer3.1.conv1           (layer3_0_relu_1,)                       {}\n",
      "call_module    layer3_1_relu          layer3.1.relu            (layer3_1_conv1,)                        {}\n",
      "call_module    layer3_1_conv2         layer3.1.conv2           (layer3_1_relu,)                         {}\n",
      "call_function  add_7                  <built-in function add>  (layer3_1_conv2, layer3_0_relu_1)        {}\n",
      "call_module    layer3_1_relu_1        layer3.1.relu            (add_7,)                                 {}\n",
      "call_module    layer3_2_conv1         layer3.2.conv1           (layer3_1_relu_1,)                       {}\n",
      "call_module    layer3_2_relu          layer3.2.relu            (layer3_2_conv1,)                        {}\n",
      "call_module    layer3_2_conv2         layer3.2.conv2           (layer3_2_relu,)                         {}\n",
      "call_function  add_8                  <built-in function add>  (layer3_2_conv2, layer3_1_relu_1)        {}\n",
      "call_module    layer3_2_relu_1        layer3.2.relu            (add_8,)                                 {}\n",
      "call_module    avgpool                avgpool                  (layer3_2_relu_1,)                       {}\n",
      "call_module    flatten                flatten                  (avgpool,)                               {}\n",
      "call_module    fc                     fc                       (flatten,)                               {}\n",
      "output         output                 output                   (fc,)                                    {}\n",
      "opcode       name                   target                 args                                       kwargs\n",
      "-----------  ---------------------  ---------------------  -----------------------------------------  --------\n",
      "placeholder  x                      x                      ()                                         {}\n",
      "call_module  conv1_1                conv1                  (x,)                                       {}\n",
      "call_module  relu_1                 relu                   (conv1_1,)                                 {}\n",
      "call_module  layer1_0_conv1_1       layer1_0_conv1         (relu_1,)                                  {}\n",
      "call_module  layer1_0_relu_2        layer1_0_relu          (layer1_0_conv1_1,)                        {}\n",
      "call_module  layer1_0_conv2_1       layer1_0_conv2         (layer1_0_relu_2,)                         {}\n",
      "call_module  add_9                  add                    (layer1_0_conv2_1, relu_1)                 {}\n",
      "call_module  layer1_0_relu_3        layer1_0_relu_1        (add_9,)                                   {}\n",
      "call_module  layer1_1_conv1_1       layer1_1_conv1         (layer1_0_relu_3,)                         {}\n",
      "call_module  layer1_1_relu_2        layer1_1_relu          (layer1_1_conv1_1,)                        {}\n",
      "call_module  layer1_1_conv2_1       layer1_1_conv2         (layer1_1_relu_2,)                         {}\n",
      "call_module  add_10                 add_1                  (layer1_1_conv2_1, layer1_0_relu_3)        {}\n",
      "call_module  layer1_1_relu_3        layer1_1_relu_1        (add_10,)                                  {}\n",
      "call_module  layer1_2_conv1_1       layer1_2_conv1         (layer1_1_relu_3,)                         {}\n",
      "call_module  layer1_2_relu_2        layer1_2_relu          (layer1_2_conv1_1,)                        {}\n",
      "call_module  layer1_2_conv2_1       layer1_2_conv2         (layer1_2_relu_2,)                         {}\n",
      "call_module  add_11                 add_2                  (layer1_2_conv2_1, layer1_1_relu_3)        {}\n",
      "call_module  layer1_2_relu_3        layer1_2_relu_1        (add_11,)                                  {}\n",
      "call_module  layer2_0_conv1_1       layer2_0_conv1         (layer1_2_relu_3,)                         {}\n",
      "call_module  layer2_0_relu_2        layer2_0_relu          (layer2_0_conv1_1,)                        {}\n",
      "call_module  layer2_0_conv2_1       layer2_0_conv2         (layer2_0_relu_2,)                         {}\n",
      "call_module  layer2_0_downsample_1  layer2_0_downsample_0  (layer1_2_relu_3,)                         {}\n",
      "call_module  add_12                 add_3                  (layer2_0_conv2_1, layer2_0_downsample_1)  {}\n",
      "call_module  layer2_0_relu_3        layer2_0_relu_1        (add_12,)                                  {}\n",
      "call_module  layer2_1_conv1_1       layer2_1_conv1         (layer2_0_relu_3,)                         {}\n",
      "call_module  layer2_1_relu_2        layer2_1_relu          (layer2_1_conv1_1,)                        {}\n",
      "call_module  layer2_1_conv2_1       layer2_1_conv2         (layer2_1_relu_2,)                         {}\n",
      "call_module  add_13                 add_4                  (layer2_1_conv2_1, layer2_0_relu_3)        {}\n",
      "call_module  layer2_1_relu_3        layer2_1_relu_1        (add_13,)                                  {}\n",
      "call_module  layer2_2_conv1_1       layer2_2_conv1         (layer2_1_relu_3,)                         {}\n",
      "call_module  layer2_2_relu_2        layer2_2_relu          (layer2_2_conv1_1,)                        {}\n",
      "call_module  layer2_2_conv2_1       layer2_2_conv2         (layer2_2_relu_2,)                         {}\n",
      "call_module  add_14                 add_5                  (layer2_2_conv2_1, layer2_1_relu_3)        {}\n",
      "call_module  layer2_2_relu_3        layer2_2_relu_1        (add_14,)                                  {}\n",
      "call_module  layer3_0_conv1_1       layer3_0_conv1         (layer2_2_relu_3,)                         {}\n",
      "call_module  layer3_0_relu_2        layer3_0_relu          (layer3_0_conv1_1,)                        {}\n",
      "call_module  layer3_0_conv2_1       layer3_0_conv2         (layer3_0_relu_2,)                         {}\n",
      "call_module  layer3_0_downsample_1  layer3_0_downsample_0  (layer2_2_relu_3,)                         {}\n",
      "call_module  add_15                 add_6                  (layer3_0_conv2_1, layer3_0_downsample_1)  {}\n",
      "call_module  layer3_0_relu_3        layer3_0_relu_1        (add_15,)                                  {}\n",
      "call_module  layer3_1_conv1_1       layer3_1_conv1         (layer3_0_relu_3,)                         {}\n",
      "call_module  layer3_1_relu_2        layer3_1_relu          (layer3_1_conv1_1,)                        {}\n",
      "call_module  layer3_1_conv2_1       layer3_1_conv2         (layer3_1_relu_2,)                         {}\n",
      "call_module  add_16                 add_7                  (layer3_1_conv2_1, layer3_0_relu_3)        {}\n",
      "call_module  layer3_1_relu_3        layer3_1_relu_1        (add_16,)                                  {}\n",
      "call_module  layer3_2_conv1_1       layer3_2_conv1         (layer3_1_relu_3,)                         {}\n",
      "call_module  layer3_2_relu_2        layer3_2_relu          (layer3_2_conv1_1,)                        {}\n",
      "call_module  layer3_2_conv2_1       layer3_2_conv2         (layer3_2_relu_2,)                         {}\n",
      "call_module  add_17                 add_8                  (layer3_2_conv2_1, layer3_1_relu_3)        {}\n",
      "call_module  layer3_2_relu_3        layer3_2_relu_1        (add_17,)                                  {}\n",
      "call_module  avgpool_1              avgpool                (layer3_2_relu_3,)                         {}\n",
      "call_module  flatten_1              flatten                (avgpool_1,)                               {}\n",
      "call_module  fc_1                   fc                     (flatten_1,)                               {}\n",
      "output       output                 output                 (fc_1,)                                    {}\n"
     ]
    }
   ],
   "source": [
    "# 采用qconfig_file中定义的量化参数\n",
    "qconfig_file = \"qconfig.yaml\"\n",
    "qconfig = parse_qconfig(qconfig_file)\n",
    "# 定义量化模型\n",
    "qmodel = QuantModel(model, config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Calibration\n",
    "- Prepare Dataset for calibration,here we adopt 256 images\n",
    "- Calibrate Model using calibration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set calibration\n",
    "qmodel.prepare_calibration()\n",
    "# Forward Calibrate\n",
    "calibration_size = 256\n",
    "cur_size = 0\n",
    "if torch.cuda.is_available():\n",
    "    qmodel.cuda()\n",
    "for data,target in trainloader:\n",
    "    if torch.cuda.is_available():\n",
    "        data,target = data.cuda(),target.cuda()\n",
    "    res = qmodel(data)\n",
    "    cur_size += data.shape[0]\n",
    "    if cur_size >= calibration_size:\n",
    "        break\n",
    "qmodel.calc_qparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Test Quant Model\n",
    "- Test PTQ Model in Testset, compare with float model\n",
    "- export onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Quant Model on the 10000 test images: 91.36 %\n",
      "Accuracy of the Float   Model on the 10000 test images: 91.42 %\n"
     ]
    }
   ],
   "source": [
    "# Set Quantization\n",
    "qmodel.set_quant(w_quant=True,a_quant=True)\n",
    "correct = 0\n",
    "total = 0\n",
    "qmodel.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        image,labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            image,labels= image.cuda(),labels.cuda()\n",
    "        outputs = qmodel(image)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "acc1 = 100 * correct / total\n",
    "print(f'Accuracy of the Quant Model on the 10000 test images: {acc1} %')\n",
    "print(f'Accuracy of the Float   Model on the 10000 test images: {pretrain_acc1} %')\n",
    "\n",
    "# 导出onnx\n",
    "qmodel.export_onnx(torch.randn(1, 3, 224, 224), name=\"qresnet20.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7bd8c85bb856bc21c6c4a9fae961e27ba2d754618a567ef27c67dee68f76882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
